{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f331d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_ai_integration_test.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f455da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kakarot/Developer/Project/resume-builder\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "189f6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables from .env file using python-dotenv library\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f61cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resume_optimizer.core.ats_optimizer.optimizer import ATSOptimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e90bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759312848.774703 4291644 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "opt = ATSOptimizer(os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4f42e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResumeData(contact_info=ContactInfo(name='Ashish Surve', email='surve.ashish@outlook.com', phone=None, address=None, linkedin='linkedin.com/in/ashish-surve', github='github.com/ashish-surve'), summary='Product-focused Data Scientist with 6+ years building and shipping ML systems for CPG, retail and real-estate domains. Expert in demand forecasting, anomaly detection, and trade-promotion optimization — shipped production solutions that trained 300k models on multi-TB data and delivered $2.4M in client savings. Strong end-to-end ownership: problem framing, distributed model development (PySpark/Databricks), API deployment, monitoring, and stakeholder enablement.', skills=['Artificial Intelligence', 'Docker', 'Pandas', 'Numpy', 'Pytorch', 'Aws', 'Scikit-Learn', 'Data Science', 'Python', 'Tensorflow', 'Git', 'Machine Learning', 'Azure', 'Mongodb', 'Mysql'], experience=[], education=[], certifications=[], languages=[], raw_text='Ashish Surve\\nć surve.ashish@outlook.com | Ħ +91 70400 71239\\na github.com/ashish-surve | ] linkedin.com/in/ashish-surve\\nSummary\\nProduct-focused Data Scientist with 6+ years building and shipping ML systems for CPG, retail and real-estate domains.\\nExpert in demand forecasting, anomaly detection, and trade-promotion optimization — shipped production solutions that\\ntrained 300k models on multi-TB data and delivered $2.4M in client savings. Strong end-to-end ownership: problem framing,\\ndistributed model development (PySpark/Databricks), API deployment, monitoring, and stakeholder enablement.\\nSkills\\nCore: Forecasting, Time Series, Anomaly Detection, Trade Promotion Optimization, Product ML, MLOps, Model Monitoring\\nLanguages & Frameworks:Python, SQL, PySpark, Pandas, NumPy, scikit-learn, PyTorch, TensorFlow, FastAPI, Streamlit\\nData & Infra: Databricks, Spark, Dask, MongoDB, MySQL, Azure, AWS, Docker, CI/CD, Git, Hyperparameter tuning,\\nDistributed training\\nNLP / LLMs:RAG, Vector stores, OpenAI, Hugging Face, Semantic search\\nTools: SHAP, PyOD, Vaex, Plotly, Streamlit, FastAPI, pre-commit\\nWork Experience\\nGlobant May 2024 – Present\\nSenior Data Scientist (Team Lead)\\n• Led cross-functional delivery team (Big 4):Directed data science and engineering efforts, partnering with DevOps\\nand solution architects to implement scalable, maintainable Data Engineering and ML best practices.\\n• Enabled SaaS-readiness for payroll fraud detection:Architected end-to-end, cloud-agnostic ML and data pipelines\\nto support a future SaaS offering; reduced development time for new clients by40%.\\n• Built configurable anomaly detection platform:Delivered a multi-algorithm, config-driven system integrated with\\nclient front-end/back-end stacks to reduce manual intervention and simplify onboarding.\\n• Innovated correlation-based detection:Implemented PySpark modules for payroll-component correlation analysis\\nto improve anomaly discrimination and robustness across payroll complexities.\\n• Ensured cloud vendor-independence:Engineered portable pipelines and orchestration usingAzure ML, Synapse,\\nPrefect, Docker, Azure Functions, AWS SageMaker, enabling multi-cloud deployment options for clients.\\n• Technologies: Python, PySpark, SQL, Azure ML, Synapse, Prefect, Docker, Azure Functions, AWS SageMaker, Azure\\nSDK v2\\nTiger Analytics May 2021 – May 2024\\nData Scientist (Project Lead)\\n• Demand Forecasting Accelerator: Designed and productionized a PySpark-based forecasting accelerator\\n(Databricks) with a0.94 parallelization factorused across 20+ projects. Reduced model development and de-\\nployment time from weeks to days and standardized configuration-driven deployments.\\n• Out-of-Stock Prevention (US):Led a production solution that trained300,000 modelsover a2TB dataset with auto-\\nmated backtesting and HPO; outcome:$2.4M cost avoidance by reducing stockouts and expiry losses. Built simplified\\nforecast views for non-technical managers.\\n• Out-of-Stock Prevention (Russia): Headed implementation for a major CPG client; engineered50,000 models\\nachieving 84% accuracy on high-quality segments and introduced feature-engineering strategies to improve low-\\nquality data performance. Implemented Temporal Fusion Transformer for cross-learning and gained+17% uplift vs\\nprevious top models.\\n• Anomaly Detection Accelerator:Co-developed a time-series anomaly detection system with automatic root-cause\\nanalysis (RCA) that reduced manual triage and improved incident response for supply-chain signals.\\n• Trade Promotion Optimization (UK):Built a TPO tool with scenario planning and profit-pool dashboards to identify\\nhistorically successful promotions and recommend future strategies per category.\\n• Cashflow Forecasting:Led delivery of a cashflow forecasting pipeline to provide stakeholders with short-term financial\\nvisibility and scenario analysis for planning.\\n• LLM Marketing Assistant:Prototyped an LLM + RAG agent to answer structured marketing queries for CMOs; inte-\\ngrated vector store retrieval and QA over tables — awarded 2nd place in internal hackathon.\\n• Technologies: Python, PySpark, Databricks, TensorFlow, PyTorch, FastAPI, Streamlit, OpenAI, Hugging Face, RAG,\\nSQL, Docker\\nvCreaTek Consulting Services Pvt Ltd Jun 2020 – May 2021\\nData Scientist & Developer\\n• Built Decision Support Systems for real-estate clients delivering KPIs and dashboards that informed strategic invest-\\nments and pricing decisions.\\n• Led API Gateway and Developer Portal productionization (Golang + Looker) to enable external partners to consume\\ninternal APIs securely.\\n• Automated repetitive data tasks and mentored junior engineers; earned RapidMiner Grandmaster recognition and 6\\ncertifications.\\n• Technologies: Golang, Looker, Python, RapidMiner, SQL\\nPersistent Systems Jan 2020 – Jul 2020\\nSoftware Engineer Apprentice\\n• Led an 8-member team to deliver a COVID-19 pedestrian detection and homography-correction pipeline (computer\\nvision) within one month; focused on rapid prototyping and production-readiness.\\n• Completed industry certifications and internal exams (AI/ML fundamentals).\\n• Technologies: Python, OpenCV, MTCNN, FaceNet\\nIoTIoT.in Sep 2019 – Jan 2020\\nArtificial Intelligence Intern\\n• Implemented face-recognition pipeline using MTCNN + FaceNet and optimised inference pipelines for edge deploy-\\nment; achieved top rank in an AI-Python Hackerrank among IIT participants.\\n• Technologies: MTCNN, FaceNet, Python\\nEducation\\nDr. D. Y. Patil Institute of Technology, SPPU, Pune B.E. Computer Science\\nFirst Class with Distinction\\nRelevant coursework: Machine Learning, Data Science Statistics, Big Data Analytics, Probability, Discrete Mathematics\\nSelected Projects\\n• Demand Forecasting Accelerator (github.com/ashish-surve): End-to-end forecasting framework that standardizes\\nconfig-driven model training, backtesting and deployment across multiple clients. Includes PySpark pipelines and\\nautomated HPO jobs.\\n• Out-of-Stock Prevention Demo:Repro notebook illustrating per-SKU forecasting with backtesting and scenario anal-\\nysis (links and docs on GitHub).\\n• LLM Marketing Assistant Prototype:RAG-based agent integrating vector store retrieval and structured-table QA to\\nsurface marketing insights to non-technical users.\\nAwards & Certifications\\n• RapidMiner Grandmaster (6 certifications)\\n• 2nd place — Tiger Analytics internal Hackathon (LLM Marketing Assistant)\\n• 1st rank — AI-Python Hackerrank (IoTIoT.in internship contest)\\n• Certifications: Microsoft AI/ML fundamentals (internal exams listed)', file_path=PosixPath('/Users/kakarot/Developer/Project/resume-builder/data/input/resumes/!Lead Data Scientist Resume Ashish Surve 6.pdf'), file_type=<FileType.PDF: 'pdf'>, created_at=datetime.datetime(2025, 10, 1, 15, 9, 31, 520289))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# load resume data from a file \n",
    "from resume_optimizer.core.resume_parser.parser import ResumeParserFactory\n",
    "\n",
    "parser = ResumeParserFactory.create_parser()\n",
    "\n",
    "\n",
    "path_file = Path(os.getcwd(), 'data', 'input', 'resumes', '!Lead Data Scientist Resume Ashish Surve 6.pdf')\n",
    "resume_data = parser.parse(path_file)\n",
    "print(resume_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f54a3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = \"\"\"\n",
    "About the job\n",
    "Position Overview Job Title: Director: AI Tech Lead Location: Pune Job Summary\n",
    "\n",
    "We are seeking an experienced AI Technology Lead to lead our Artificial Intelligence(AI) strategy, development, and implementation across the function. The successful candidate will work in collaboration with other business functions and departments and will be responsible for designing, developing, and deploying AI solutions, which are in-line with bank’s AI strategy and which drive business value, improve operational efficiency, and enhance customer experience.\n",
    "\n",
    "What we’ll offer you:\n",
    "\n",
    "As part of our flexible scheme, here are just some of the benefits that you’ll enjoy\n",
    "\n",
    "Best in class leave policy\n",
    "Gender neutral parental leaves\n",
    "100% reimbursement under child care assistance benefit (gender neutral)\n",
    "Flexible working arrangements\n",
    "Sponsorship for Industry relevant certifications and education\n",
    "Employee Assistance Program for you and your family members\n",
    "Comprehensive Hospitalization Insurance for you and your dependents\n",
    "Accident and Term life Insurance\n",
    "Complementary Health screening for 35 yrs. and above\n",
    "\n",
    "\n",
    "Key Responsibilities: Strategic Planning and Leadership\n",
    "\n",
    "Develop and execute the AI strategy aligned with the bank's overall technology vision and objectives.\n",
    "Collaborate with cross-functional teams to identify opportunities for AI adoption and prioritize projects.\n",
    "Provide technical leadership and guidance to the AI development team.\n",
    "\n",
    "\n",
    "AI Development and Deployment\n",
    "\n",
    "Design, develop, and deploy AI models and algorithms that meet business requirements and regulatory standards.\n",
    "Lead the development of AI-powered solutions for various business domains, including P&L accounting, and process automation.\n",
    "Ensure seamless integration with existing systems and data platforms.\n",
    "\n",
    "\n",
    "Research and Innovation\n",
    "\n",
    "Stay up-to-date with the latest advancements in AI and machine learning (ML) technologies.\n",
    "Collaborate with other divisions and central AI teams to identify emerging trends and technologies that can benefit the division.\n",
    "Develop proof-of-concept projects and pilots to explore new AI applications.\n",
    "\n",
    "\n",
    "Data Management and Analytics\n",
    "\n",
    "Ensure data quality, security, and governance for AI model development and deployment.\n",
    "Collaborate with the data analytics team to develop and implement data-driven insights and recommendations.\n",
    "\n",
    "\n",
    "Regulatory Compliance and Risk Management\n",
    "\n",
    "Ensure that all AI solutions comply with regulatory requirements and industry standards.\n",
    "Conduct regular risk assessments to identify potential vulnerabilities and mitigate them proactively.\n",
    "\n",
    "\n",
    "Collaboration and Communication\n",
    "\n",
    "Collaborate with stakeholders across the organization to ensure alignment and buy-in for AI initiatives.\n",
    "Communicate technical results and recommendations to non-technical stakeholders through clear and concise language.\n",
    "\n",
    "\n",
    "Requirements Education and Experience\n",
    "\n",
    "Bachelor's or Master's degree in Computer Science, Mathematics, Statistics, or a related field.\n",
    "Minimum 5+ years of experience in AI, ML, or data science, with at least 2+ years in a leadership role.\n",
    "\n",
    "\n",
    "Technical Skills\n",
    "\n",
    "Proficiency in Python, R, or other programming languages.\n",
    "Experience with deep learning frameworks (e.g., TensorFlow, PyTorch).\n",
    "Familiarity with natural language processing (NLP) and LLM architecture.\n",
    "Knowledge of data management platforms (e.g., Hadoop, Spark, BQ, ADX).\n",
    "\n",
    "\n",
    "Leadership and Management\n",
    "\n",
    "Proven track record of leading high-performing teams and driving results-oriented projects.\n",
    "Experience with Agile methodologies and Scrum frameworks.\n",
    "Strong communication and interpersonal skills.\n",
    "\n",
    "\n",
    "Good to Have\n",
    "\n",
    "Certifications: Relevant certifications in Data Science, Analytics, etc.\n",
    "\n",
    "Industry Knowledge: Experience working in the financial services industry, preferably in a global bank setting.\n",
    "\n",
    "Cloud Computing: Familiarity with cloud computing platforms (e.g., GCP, AWS, Azure).\n",
    "\n",
    "If you are an AI expert looking for a challenging role that will drive innovation and growth, please submit your application.\n",
    "\n",
    "How We’ll Support You\n",
    "\n",
    "  Training and development to help you excel in your career \n",
    " Flexible working to assist you balance your personal priorities \n",
    " Coaching and support from experts in your team \n",
    " A culture of continuous learning to aid progression \n",
    " A range of flexible benefits that you can tailor to suit your needs \n",
    "\n",
    "\n",
    "About Us And Our Teams\n",
    "\n",
    "Please visit our company website for further information:\n",
    "\n",
    " https://www.db.com/company/company.htm \n",
    "\n",
    "We strive for a culture in which we are empowered to excel together every day. This includes acting responsibly, thinking commercially, taking initiative and working collaboratively.\n",
    "\n",
    "Together we share and celebrate the successes of our people. Together we are Deutsche Bank Group.\n",
    "\n",
    "We welcome applications from all people and promote a positive, fair and inclusive work environment.\n",
    "\"\"\"\n",
    "\n",
    "company_name = \"Deutsche Bank\"\n",
    "applicant_name = \"Ashish Surve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a7c6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resume_optimizer.core.job_analyzer.analyzer import JobDescriptionAnalyzer\n",
    "\n",
    "ja = JobDescriptionAnalyzer()\n",
    "job_data = ja.analyze(job_data, company_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0e88ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to optimize summary with Gemini: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "Failed to enhance skills with Gemini: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "Failed to generate recommendations with Gemini: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizationResult(original_score=75.97129186602871, optimized_score=52.16427432216906, improvements=['Professional summary optimized with relevant keywords and stronger positioning', 'Experience descriptions enhanced with action verbs and quantifiable achievements', 'Skills section reorganized to highlight job-relevant technical competencies', 'Content structure improved for better ATS parsing and readability'], missing_keywords=['leadership', 'teams', 'machine learning', 'agile', 'pytorch', 'develop', 'data science', 'management', 'working', 'artificial intelligence', 'scrum', 'business', 'bank', 'lead', 'algorithms', 'ai strategy', 'technical', 'flexible', 'tensorflow', 'collaborate', 'ensure'], suggested_additions=[], ats_compliance_score=87.99999999999999, readability_score=0.0, recommendations=['Consider incorporating more job-relevant keywords naturally into your resume', 'Use standard fonts like Arial, Calibri, or Times New Roman', 'Incorporate relevant keywords naturally into your experience descriptions', 'Add a skills section with technical keywords', \"Consider incorporating 'leadership' in your job descriptions if relevant\", \"Consider incorporating 'teams' in your job descriptions if relevant\", \"Consider incorporating 'develop' in your job descriptions if relevant\", 'Incorporate more job-specific keywords naturally throughout your resume', 'Streamline content to focus on most relevant and impactful achievements'], optimized_resume=ResumeData(contact_info=ContactInfo(name='Ashish Surve', email='surve.ashish@outlook.com', phone=None, address=None, linkedin='linkedin.com/in/ashish-surve', github='github.com/ashish-surve'), summary='Product-focused Data Scientist with 6+ years building and shipping ML systems for CPG, retail and real-estate domains. Expert in demand forecasting, anomaly detection, and trade-promotion optimization — shipped production solutions that trained 300k models on multi-TB data and delivered $2.4M in client savings. Strong end-to-end ownership: problem framing, distributed model development (PySpark/Databricks), API deployment, monitoring, and stakeholder enablement.', skills=['Artificial Intelligence', 'Docker', 'Pandas', 'Numpy', 'Pytorch', 'Aws', 'Scikit-Learn', 'Data Science', 'Python', 'Tensorflow', 'Git', 'Machine Learning', 'Azure', 'Mongodb', 'Mysql'], experience=[], education=[], certifications=[], languages=[], raw_text='Ashish Surve\\nEmail: surve.ashish@outlook.com\\nLinkedIn: linkedin.com/in/ashish-surve\\n\\nPROFESSIONAL SUMMARY\\nProduct-focused Data Scientist with 6+ years building and shipping ML systems for CPG, retail and real-estate domains. Expert in demand forecasting, anomaly detection, and trade-promotion optimization — shipped production solutions that trained 300k models on multi-TB data and delivered $2.4M in client savings. Strong end-to-end ownership: problem framing, distributed model development (PySpark/Databricks), API deployment, monitoring, and stakeholder enablement.\\n\\nTECHNICAL SKILLS\\n• Artificial Intelligence\\n• Docker\\n• Pandas\\n• Numpy\\n• Pytorch\\n• Aws\\n• Scikit-Learn\\n• Data Science\\n• Python\\n• Tensorflow\\n• Git\\n• Machine Learning\\n• Azure\\n• Mongodb\\n• Mysql\\n', file_path=PosixPath('/Users/kakarot/Developer/Project/resume-builder/data/input/resumes/!Lead Data Scientist Resume Ashish Surve 6.pdf'), file_type=<FileType.PDF: 'pdf'>, created_at=datetime.datetime(2025, 10, 1, 15, 9, 31, 520289)), status=<OptimizationStatus.COMPLETED: 'completed'>, created_at=datetime.datetime(2025, 10, 1, 15, 30, 52, 271725))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.optimize(resume_data=resume_data,\n",
    "             job_data=job_data,\n",
    "             applicant_name=applicant_name,\n",
    "             company_name=company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a56d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-builder (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
